"""
California Algorithm for Automatic Incident Detection
Classic rule-based approach for comparison with ML model
OPTIMIZED VERSION with progress tracking for large datasets
"""

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
import time
import json
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm

import config


class CaliforniaAlgorithm:
    """California Algorithm #8 - Classic incident detection"""
    
    def __init__(self, 
                 threshold_occupancy: float = 0.25,
                 threshold_speed: float = 18.0,
                 min_duration: int = 30,
                 detector_weights: dict = None):
        self.threshold_occupancy = threshold_occupancy
        self.threshold_speed = threshold_speed
        self.min_duration = min_duration
        
        num_dets = len(config.detector_edges)
        self.detector_weights = detector_weights or {edge: 1.0/num_dets for edge in config.detector_edges}
        
        self.alarm_start_times = {}
    
    def check_detector_condition(self, occupancy: float, speed: float) -> bool:
        """Check if a single detector triggers alarm conditions"""
        return (occupancy > self.threshold_occupancy) and (speed < self.threshold_speed)
    
    def predict_scenario(self, scenario_df: pd.DataFrame) -> np.ndarray:
        """Predict for a single scenario - optimized"""
        predictions = np.zeros(len(scenario_df), dtype=int)
        alarm_active = False
        alarm_start_time = None
        
        for i, (idx, row) in enumerate(scenario_df.iterrows()):
            current_time = row['sim_time']
            
            # Check each detector
            detector_alarms = []
            for det in config.detector_edges:
                occupancy = row[f'{det}_occupancy']
                speed = row[f'{det}_speed_mean']
                
                if self.check_detector_condition(occupancy, speed):
                    detector_alarms.append(self.detector_weights.get(det, 0.33))
            
            alarm_score = sum(detector_alarms)
            
            if alarm_score > 0.1:
                if not alarm_active:
                    alarm_start_time = current_time
                    alarm_active = True
                
                if current_time - alarm_start_time >= self.min_duration:
                    predictions[i] = 1
            else:
                alarm_active = False
                alarm_start_time = None
        
        return predictions
    
    def predict(self, test_df: pd.DataFrame, show_progress: bool = True) -> np.ndarray:
        """Apply California Algorithm to test dataset with progress tracking"""
        print(f"\nðŸš¦ Running California Algorithm...")
        print(f"   Occupancy threshold: {self.threshold_occupancy*100:.1f}%")
        print(f"   Speed threshold: {self.threshold_speed:.1f} m/s ({self.threshold_speed*3.6:.1f} km/h)")
        print(f"   Minimum duration: {self.min_duration}s")
        
        # Get unique scenarios
        unique_scenarios = test_df['scenario_id'].unique()
        print(f"   Processing {len(unique_scenarios):,} scenarios with {len(test_df):,} total samples...")
        
        all_predictions = []
        
        # Process with progress bar
        if show_progress:
            for scenario_id in tqdm(unique_scenarios, desc="Processing scenarios", unit="scenario"):
                scenario_df = test_df[test_df['scenario_id'] == scenario_id].copy()
                scenario_df = scenario_df.sort_values('sim_time').reset_index(drop=True)
                
                scenario_predictions = self.predict_scenario(scenario_df)
                all_predictions.extend(scenario_predictions)
        else:
            for scenario_id in unique_scenarios:
                scenario_df = test_df[test_df['scenario_id'] == scenario_id].copy()
                scenario_df = scenario_df.sort_values('sim_time').reset_index(drop=True)
                
                scenario_predictions = self.predict_scenario(scenario_df)
                all_predictions.extend(scenario_predictions)
        
        return np.array(all_predictions)
    
    def predict_pre_sorted(self, sorted_df: pd.DataFrame, show_progress: bool = False) -> np.ndarray:
        """Faster prediction using pre-sorted data"""
        all_predictions = []
        unique_scenarios = sorted_df['scenario_id'].unique()
        
        if show_progress:
            iterator = tqdm(unique_scenarios, desc="Predicting", unit="scenario")
        else:
            iterator = unique_scenarios
        
        for scenario_id in iterator:
            scenario_df = sorted_df[sorted_df['scenario_id'] == scenario_id]
            all_predictions.extend(self.predict_scenario(scenario_df))
        
        return np.array(all_predictions)
    
    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray, 
                 save_results: bool = True) -> dict:
        """Evaluate California Algorithm performance"""
        print("\n" + "="*60)
        print("ðŸš¦ CALIFORNIA ALGORITHM EVALUATION")
        print("="*60)
        
        # Calculate metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # Print results
        print(f"\nPerformance Metrics:")
        print(f"  Accuracy:  {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall:    {recall:.4f}")
        print(f"  F1-score:  {f1:.4f}")
        
        print(f"\nConfusion Matrix:")
        print(f"  True Negatives:  {tn:,}")
        print(f"  False Positives: {fp:,}")
        print(f"  False Negatives: {fn:,}")
        print(f"  True Positives:  {tp:,}")
        
        detection_rate = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0
        false_alarm_rate = (fp / (fp + tn)) * 100 if (fp + tn) > 0 else 0
        
        print(f"\nKey Insights:")
        print(f"  Detection rate: {detection_rate:.1f}%")
        print(f"  False alarm rate: {false_alarm_rate:.1f}%")
        
        print(f"\nClassification Report:")
        print(classification_report(y_true, y_pred, 
                                   target_names=['No Incident', 'Incident']))
        
        # Save results
        results = {
            'algorithm': 'California Algorithm #8',
            'parameters': {
                'threshold_occupancy': self.threshold_occupancy,
                'threshold_speed_ms': self.threshold_speed,
                'threshold_speed_kmh': self.threshold_speed * 3.6,
                'min_duration_seconds': self.min_duration
            },
            'metrics': {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1)
            },
            'confusion_matrix': {
                'tn': int(tn), 'fp': int(fp), 
                'fn': int(fn), 'tp': int(tp)
            },
            'detection_rate_percent': float(detection_rate),
            'false_alarm_rate_percent': float(false_alarm_rate)
        }
        
        if save_results:
            results_file = config.results_dir / "california_algorithm_results.json"
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"\nðŸ’¾ Results saved to {results_file}")
            
            # Create comparison plot
            self._plot_confusion_matrix(cm)
        
        return results
    
    def _plot_confusion_matrix(self, cm):
        """Plot confusion matrix"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['No Incident', 'Incident'],
                   yticklabels=['No Incident', 'Incident'])
        plt.title('California Algorithm - Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        save_path = config.results_dir / "california_algorithm_confusion_matrix.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ðŸ’¾ Confusion matrix saved to {save_path}")


def calculate_practical_score(dr, far, weight_dr=0.7, weight_far=0.3, far_penalty_threshold=10.0):
    """Calculate a practical score that balances detection rate and false alarm rate"""
    dr_normalized = dr / 100.0
    
    if far <= far_penalty_threshold:
        far_penalty = far / 100.0
    else:
        far_penalty = (far_penalty_threshold / 100.0) + \
                     ((far - far_penalty_threshold) / 100.0) ** 2
    
    score = (weight_dr * dr_normalized) - (weight_far * far_penalty)
    
    return max(0, score)


def evaluate_single_config(args):
    """Worker function for parallel processing"""
    occ, spd, dur, test_df, y_true, config_num, total_configs = args
    
    # Progress indicator
    if config_num % 10 == 0:
        print(f"   Testing config {config_num}/{total_configs}...", end='\r')
    
    cal = CaliforniaAlgorithm(
        threshold_occupancy=occ, 
        threshold_speed=spd, 
        min_duration=dur
    )
    preds = cal.predict_pre_sorted(test_df, show_progress=False)
    
    # Calculate metrics
    cm = confusion_matrix(y_true, preds)
    tn, fp, fn, tp = cm.ravel()
    
    acc = accuracy_score(y_true, preds)
    prec = precision_score(y_true, preds, zero_division=0)
    rec = recall_score(y_true, preds, zero_division=0)
    f1 = f1_score(y_true, preds, zero_division=0)
    
    dr = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0
    far = (fp / (fp + tn)) * 100 if (fp + tn) > 0 else 0
    
    practical_score = calculate_practical_score(dr, far)
    
    return {
        'occ': occ,
        'spd': spd,
        'dur': dur,
        'acc': acc,
        'prec': prec,
        'rec': rec,
        'f1': f1,
        'dr': dr,
        'far': far,
        'practical_score': practical_score,
        'tp': tp,
        'fp': fp,
        'tn': tn,
        'fn': fn
    }


def inspect_and_optimize():
    print("\n" + "="*60)
    print("ðŸ” STEP 1: DATA INSPECTION")
    print("="*60)
    
    print("\nðŸ“‚ Loading test data...")
    start_load = time.time()
    test_df = pd.read_hdf(config.test_h5, key='data')
    load_time = time.time() - start_load
    print(f"   âœ… Loaded {len(test_df):,} samples in {load_time:.1f}s")
    
    # Print first 10 rows
    print("\nðŸ“„ FIRST 10 ROWS OF TEST DATA:")
    cols_to_show = ['scenario_id', 'sim_time', 'incident_active'] + \
                   [c for c in test_df.columns if 'occupancy' in c or 'speed_mean' in c][:4]
    print(test_df[cols_to_show].head(10))
    print("-" * 30)

    # Check data statistics
    print("\nðŸ“Š Computing data statistics...")
    occ_cols = [c for c in test_df.columns if 'occupancy' in c]
    spd_cols = [c for c in test_df.columns if 'speed_mean' in c]
    
    occ_max = test_df[occ_cols].max().max()
    spd_mean = test_df[spd_cols].mean().mean()
    spd_min = test_df[spd_cols].min().min()
    
    num_scenarios = test_df['scenario_id'].nunique()
    num_incidents = test_df['incident_active'].sum()
    incident_pct = (num_incidents / len(test_df)) * 100
    
    print(f"\nðŸ“ˆ Data Summary:")
    print(f"   Total samples: {len(test_df):,}")
    print(f"   Unique scenarios: {num_scenarios:,}")
    print(f"   Incident samples: {num_incidents:,} ({incident_pct:.2f}%)")
    print(f"   Max Occupancy: {occ_max:.4f}")
    print(f"   Average Speed: {spd_mean:.2f} m/s ({spd_mean*3.6:.1f} km/h)")
    print(f"   Min Speed: {spd_min:.2f} m/s ({spd_min*3.6:.1f} km/h)")

    # PRE-SORT DATA ONCE
    print("\nðŸ”„ Pre-sorting data by scenario and time...")
    start_sort = time.time()
    test_df = test_df.sort_values(['scenario_id', 'sim_time']).reset_index(drop=True)
    y_true = test_df['incident_active'].values
    sort_time = time.time() - start_sort
    print(f"   âœ… Sorted in {sort_time:.1f}s")

    print("\n" + "="*60)
    print("âš™ï¸ STEP 2: HYPERPARAMETER TUNING")
    print("="*60)
    print(f"\nUsing Practical Score = 0.7Ã—DR - 0.3Ã—FAR_penalty")
    print(f"(Prioritizes detection while heavily penalizing FAR > 10%)\n")
    
    # Create parameter combinations
    occ_range = [0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.15, 0.2]
    spd_range = [20, 25, 30, 35]
    dur_range = [1, 2, 5, 10]
    
    param_combinations = []
    for i, (occ, spd, dur) in enumerate(
        [(o, s, d) for o in occ_range for s in spd_range for d in dur_range]
    ):
        param_combinations.append((occ, spd, dur, test_df, y_true, i+1, len(occ_range)*len(spd_range)*len(dur_range)))
    
    num_workers = min(20, cpu_count())
    print(f"Testing {len(param_combinations)} parameter combinations using {num_workers} cores...")
    print(f"This may take 10-30 minutes depending on dataset size...\n")
    
    print(f"{'Occ':<6} | {'Spd':<6} | {'Dur':<5} | {'Acc':<6} | {'Prec':<6} | {'Rec':<6} | {'F1':<6} | {'DR(%)':<7} | {'FAR(%)':<7} | {'Score':<7}")
    print("-" * 95)
    
    # Run in parallel with progress tracking
    start_opt = time.time()
    with Pool(processes=num_workers) as pool:
        results = list(tqdm(
            pool.imap(evaluate_single_config, param_combinations),
            total=len(param_combinations),
            desc="Optimizing params",
            unit="config"
        ))
    opt_time = time.time() - start_opt
    
    print(f"\nâœ… Optimization complete in {opt_time/60:.1f} minutes!")
    
    # Find best configuration
    best_result = max(results, key=lambda x: x['practical_score'])
    
    # Print top 10 results
    print(f"\nðŸ† TOP 10 CONFIGURATIONS:")
    print(f"{'Rank':<5} | {'Occ':<6} | {'Spd':<6} | {'Dur':<5} | {'DR(%)':<7} | {'FAR(%)':<7} | {'F1':<6} | {'Score':<7}")
    print("-" * 75)
    
    sorted_results = sorted(results, key=lambda x: x['practical_score'], reverse=True)[:10]
    for rank, r in enumerate(sorted_results, 1):
        marker = "â­" if rank == 1 else "  "
        print(f"{marker} {rank:<3} | {r['occ']:<6.3f} | {r['spd']:<6.0f} | {r['dur']:<5.0f} | "
              f"{r['dr']:<7.2f} | {r['far']:<7.2f} | {r['f1']:<6.4f} | {r['practical_score']:<7.4f}")
    
    best_params = {
        'occ': best_result['occ'],
        'spd': best_result['spd'],
        'dur': best_result['dur']
    }
    
    print(f"\nâœ… Best Configuration Found!")
    print(f"   Params: Occ={best_params['occ']:.3f}, Spd={best_params['spd']}, Dur={best_params['dur']}")
    print(f"   Practical Score: {best_result['practical_score']:.4f}")
    print(f"   Detection Rate: {best_result['dr']:.1f}%")
    print(f"   False Alarm Rate: {best_result['far']:.1f}%")
    print(f"   F1 Score: {best_result['f1']:.4f}")

    # STEP 3: FINAL COMPARISON
    print("\n" + "="*60)
    print("ðŸ†š FINAL COMPARISON: Optimized California vs Random Forest")
    print("="*60)
    
    print("\nðŸ”„ Running final evaluation with best parameters...")
    final_cal = CaliforniaAlgorithm(
        threshold_occupancy=best_params['occ'], 
        threshold_speed=best_params['spd'], 
        min_duration=best_params['dur']
    )
    y_pred_cal = final_cal.predict_pre_sorted(test_df, show_progress=True)
    cal_res = final_cal.evaluate(y_true, y_pred_cal, save_results=True)

    # Load RF results
    try:
        print("\nðŸ“‚ Loading Random Forest results...")
        with open(config.results_dir / "metrics.json", 'r') as f:
            rf_res = json.load(f)
        
        # Calculate RF's FAR and DR
        rf_cm = rf_res['confusion_matrix']
        rf_far = (rf_cm['fp'] / (rf_cm['fp'] + rf_cm['tn'])) * 100
        rf_dr = (rf_cm['tp'] / (rf_cm['tp'] + rf_cm['fn'])) * 100
        
        print(f"\n{'Metric':<20} {'California':>12} {'Random Forest':>15} {'Improvement':>12}")
        print("-" * 65)
        
        metrics = [
            ('Accuracy', 'accuracy'), 
            ('Precision', 'precision'), 
            ('Recall', 'recall'), 
            ('F1-score', 'f1_score')
        ]
        
        for label, key in metrics:
            cal_val = cal_res['metrics'][key]
            rf_val = rf_res[key]
            diff = rf_val - cal_val
            print(f"{label:<20} {cal_val:>12.4f} {rf_val:>15.4f} {diff:>+12.4f}")
        
        # Operational metrics
        print(f"\n{'Operational Metrics':<20} {'California':>12} {'Random Forest':>15} {'Difference':>12}")
        print("-" * 65)
        print(f"{'Detection Rate %':<20} {best_result['dr']:>12.2f} {rf_dr:>15.2f} {rf_dr - best_result['dr']:>+12.2f}")
        print(f"{'False Alarm Rate %':<20} {best_result['far']:>12.2f} {rf_far:>15.2f} {rf_far - best_result['far']:>+12.2f}")
        
        # Practical scores
        cal_practical = calculate_practical_score(best_result['dr'], best_result['far'])
        rf_practical = calculate_practical_score(rf_dr, rf_far)
        print(f"{'Practical Score':<20} {cal_practical:>12.4f} {rf_practical:>15.4f} {rf_practical - cal_practical:>+12.4f}")
        
        print(f"\nðŸ’¡ Interpretation:")
        if rf_far < 5.0:
            print(f"   âœ… Random Forest has excellent FAR (<5%) - ready for deployment")
        elif rf_far < 10.0:
            print(f"   âœ… Random Forest has good FAR (<10%) - acceptable for deployment")
        else:
            print(f"   âš ï¸ Random Forest FAR is high (>10%) - may need threshold tuning")
        
        if best_result['far'] < rf_far:
            print(f"   â„¹ï¸ California has lower FAR, but misses {100 - best_result['dr']:.1f}% of incidents")
        
        print(f"\nðŸ“Š Thesis Conclusion:")
        print(f"   Random Forest achieves {rf_res['f1_score']:.1%} F1-score vs California's {best_result['f1']:.1%}")
        print(f"   RF detects {rf_dr:.1f}% of incidents with only {rf_far:.2f}% false alarms")
        print(f"   California detects {best_result['dr']:.1f}% with {best_result['far']:.2f}% false alarms")
        
    except FileNotFoundError:
        print("\nâš ï¸ Random Forest results not found in results/metrics.json")
        print("   Run model.py first to train and evaluate the Random Forest model")

    return best_params


if __name__ == "__main__":
    start_time = time.time()
    best_params = inspect_and_optimize()
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ Total execution time: {total_time/60:.1f} minutes")
