"""
California Algorithm for Automatic Incident Detection
Classic rule-based approach for comparison with ML model
OPTIMIZED VERSION with progress tracking for large datasets
"""

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
import time
import json
from multiprocessing import Pool, cpu_count
from functools import partial

import config


class CaliforniaAlgorithm:
    """California Algorithm #8 - Classic incident detection"""
    
    def __init__(self, 
                 threshold_occupancy: float = 0.25,
                 threshold_speed: float = 18.0,
                 min_duration: int = 30,
                 detector_weights: dict = None):
        self.threshold_occupancy = threshold_occupancy
        self.threshold_speed = threshold_speed
        self.min_duration = min_duration
        
        num_dets = len(config.detector_edges)
        self.detector_weights = detector_weights or {edge: 1.0/num_dets for edge in config.detector_edges}
        
        self.alarm_start_times = {}
    
    def check_detector_condition(self, occupancy: float, speed: float) -> bool:
        """Check if a single detector triggers alarm conditions"""
        return (occupancy > self.threshold_occupancy) and (speed < self.threshold_speed)
    
    def predict_scenario(self, scenario_df: pd.DataFrame) -> np.ndarray:
        """Predict for a single scenario - optimized"""
        predictions = np.zeros(len(scenario_df), dtype=int)
        alarm_active = False
        alarm_start_time = None
        
        for i, (idx, row) in enumerate(scenario_df.iterrows()):
            current_time = row['sim_time']
            
            # Check each detector
            detector_alarms = []
            for det in config.detector_edges:
                occupancy = row[f'{det}_occupancy']
                speed = row[f'{det}_speed_mean']
                
                if self.check_detector_condition(occupancy, speed):
                    detector_alarms.append(self.detector_weights.get(det, 0.33))
            
            alarm_score = sum(detector_alarms)
            
            if alarm_score > 0.1:
                if not alarm_active:
                    alarm_start_time = current_time
                    alarm_active = True
                
                if current_time - alarm_start_time >= self.min_duration:
                    predictions[i] = 1
            else:
                alarm_active = False
                alarm_start_time = None
        
        return predictions
    
    def predict_pre_sorted(self, sorted_df: pd.DataFrame) -> np.ndarray:
        """Faster prediction using pre-sorted data"""
        all_predictions = []
        unique_scenarios = sorted_df['scenario_id'].unique()
        
        for scenario_id in unique_scenarios:
            scenario_df = sorted_df[sorted_df['scenario_id'] == scenario_id]
            all_predictions.extend(self.predict_scenario(scenario_df))
        
        return np.array(all_predictions)
    
    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray, 
                 save_results: bool = True) -> dict:
        """Evaluate California Algorithm performance"""
        print("\n" + "="*60)
        print("ðŸš¦ CALIFORNIA ALGORITHM EVALUATION")
        print("="*60)
        
        # Calculate metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # Print results
        print(f"\nPerformance Metrics:")
        print(f"  Accuracy:  {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall:    {recall:.4f}")
        print(f"  F1-score:  {f1:.4f}")
        
        print(f"\nConfusion Matrix:")
        print(f"  True Negatives:  {tn:,}")
        print(f"  False Positives: {fp:,}")
        print(f"  False Negatives: {fn:,}")
        print(f"  True Positives:  {tp:,}")
        
        detection_rate = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0
        false_alarm_rate = (fp / (fp + tn)) * 100 if (fp + tn) > 0 else 0
        
        print(f"\nKey Insights:")
        print(f"  Detection rate: {detection_rate:.1f}%")
        print(f"  False alarm rate: {false_alarm_rate:.1f}%")
        
        print(f"\nClassification Report:")
        print(classification_report(y_true, y_pred, 
                                   target_names=['No Incident', 'Incident']))
        
        # Save results
        results = {
            'algorithm': 'California Algorithm #8',
            'parameters': {
                'threshold_occupancy': self.threshold_occupancy,
                'threshold_speed_ms': self.threshold_speed,
                'threshold_speed_kmh': self.threshold_speed * 3.6,
                'min_duration_seconds': self.min_duration
            },
            'metrics': {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1)
            },
            'confusion_matrix': {
                'tn': int(tn), 'fp': int(fp), 
                'fn': int(fn), 'tp': int(tp)
            },
            'detection_rate_percent': float(detection_rate),
            'false_alarm_rate_percent': float(false_alarm_rate)
        }
        
        if save_results:
            results_file = config.results_dir / "california_algorithm_results.json"
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"\nðŸ’¾ Results saved to {results_file}")
            
            # Create comparison plot
            self._plot_confusion_matrix(cm)
        
        return results
    
    def _plot_confusion_matrix(self, cm):
        """Plot confusion matrix"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['No Incident', 'Incident'],
                   yticklabels=['No Incident', 'Incident'])
        plt.title('California Algorithm - Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        save_path = config.results_dir / "california_algorithm_confusion_matrix.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ðŸ’¾ Confusion matrix saved to {save_path}")

    def evaluate_california_per_scenario(
        df: pd.DataFrame,
        min_confirmed_steps: int = 3
    ):
        """
        Scenario-level evaluation of California Algorithm
        """

        results = []

        for scenario_id, group in df.groupby("scenario_id"):
            y_true = group["incident_active"].max()

            alarms = group["alarm"].values

            detected = alarms.sum() >= min_confirmed_steps

            # Time to detection (if applicable)
            ttd = None
            if y_true == 1 and detected:
                incident_start = group[group["incident_active"] == 1]["sim_time"].iloc[0]
                first_alarm_time = group.loc[group["alarm"] == 1, "sim_time"].iloc[0]
                ttd = first_alarm_time - incident_start

            results.append({
                "scenario_id": scenario_id,
                "incident": int(y_true),
                "detected": int(detected),
                "time_to_detection": ttd
            })

        scenario_df = pd.DataFrame(results)

        tp = ((scenario_df.incident == 1) & (scenario_df.detected == 1)).sum()
        fn = ((scenario_df.incident == 1) & (scenario_df.detected == 0)).sum()
        fp = ((scenario_df.incident == 0) & (scenario_df.detected == 1)).sum()
        tn = ((scenario_df.incident == 0) & (scenario_df.detected == 0)).sum()

        print("\nðŸ“Š CALIFORNIA ALGORITHM â€” SCENARIO METRICS")
        print("=" * 60)
        print(f"TP: {tp}, FN: {fn}, FP: {fp}, TN: {tn}")
        print(f"Scenario FAR: {fp / (fp + tn + 1e-9):.2%}")
        print(f"Detection Rate: {tp / (tp + fn + 1e-9):.2%}")

        if scenario_df["time_to_detection"].notna().any():
            print(f"Median Time-to-Detection: {scenario_df['time_to_detection'].median():.1f}s")

        return scenario_df


def calculate_practical_score(dr, far, weight_dr=0.7, weight_far=0.3, far_penalty_threshold=10.0):
    """Calculate a practical score that balances detection rate and false alarm rate"""
    dr_normalized = dr / 100.0
    
    if far <= far_penalty_threshold:
        far_penalty = far / 100.0
    else:
        far_penalty = (far_penalty_threshold / 100.0) + \
                     ((far - far_penalty_threshold) / 100.0) ** 2
    
    score = (weight_dr * dr_normalized) - (weight_far * far_penalty)
    
    return max(0, score)


def evaluate_single_config(args):
    """Worker function for parallel processing - NO PROGRESS BAR"""
    occ, spd, dur, test_df, y_true = args
    
    cal = CaliforniaAlgorithm(
        threshold_occupancy=occ, 
        threshold_speed=spd, 
        min_duration=dur
    )
    preds = cal.predict_pre_sorted(test_df)
    
    # Calculate metrics
    cm = confusion_matrix(y_true, preds)
    tn, fp, fn, tp = cm.ravel()
    
    acc = accuracy_score(y_true, preds)
    prec = precision_score(y_true, preds, zero_division=0)
    rec = recall_score(y_true, preds, zero_division=0)
    f1 = f1_score(y_true, preds, zero_division=0)
    
    dr = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0
    far = (fp / (fp + tn)) * 100 if (fp + tn) > 0 else 0
    
    practical_score = calculate_practical_score(dr, far)
    
    return {
        'occ': occ,
        'spd': spd,
        'dur': dur,
        'acc': acc,
        'prec': prec,
        'rec': rec,
        'f1': f1,
        'dr': dr,
        'far': far,
        'practical_score': practical_score,
        'tp': tp,
        'fp': fp,
        'tn': tn,
        'fn': fn
    }


def inspect_and_optimize():
    print("\n" + "="*60)
    print("ðŸ” STEP 1: DATA INSPECTION")
    print("="*60)
    
    print("\nðŸ“‚ Loading test data...")
    start_load = time.time()
    test_df = pd.read_hdf(config.test_h5, key='data')
    load_time = time.time() - start_load
    print(f"   âœ… Loaded {len(test_df):,} samples in {load_time:.1f}s")
    
    # Print first 10 rows
    print("\nðŸ“„ FIRST 10 ROWS OF TEST DATA:")
    cols_to_show = ['scenario_id', 'sim_time', 'incident_active'] + \
                   [c for c in test_df.columns if 'occupancy' in c or 'speed_mean' in c][:4]
    print(test_df[cols_to_show].head(10))
    print("-" * 30)

    # Check data statistics
    print("\nðŸ“Š Computing data statistics...")
    occ_cols = [c for c in test_df.columns if 'occupancy' in c]
    spd_cols = [c for c in test_df.columns if 'speed_mean' in c]
    
    occ_max = test_df[occ_cols].max().max()
    spd_mean = test_df[spd_cols].mean().mean()
    spd_min = test_df[spd_cols].min().min()
    
    num_scenarios = test_df['scenario_id'].nunique()
    num_incidents = test_df['incident_active'].sum()
    incident_pct = (num_incidents / len(test_df)) * 100
    
    print(f"\nðŸ“ˆ Data Summary:")
    print(f"   Total samples: {len(test_df):,}")
    print(f"   Unique scenarios: {num_scenarios:,}")
    print(f"   Incident samples: {num_incidents:,} ({incident_pct:.2f}%)")
    print(f"   Max Occupancy: {occ_max:.4f}")
    print(f"   Average Speed: {spd_mean:.2f} m/s ({spd_mean*3.6:.1f} km/h)")
    print(f"   Min Speed: {spd_min:.2f} m/s ({spd_min*3.6:.1f} km/h)")

    # PRE-SORT DATA ONCE
    print("\nðŸ”„ Pre-sorting data by scenario and time...")
    start_sort = time.time()
    test_df = test_df.sort_values(['scenario_id', 'sim_time']).reset_index(drop=True)
    y_true = test_df['incident_active'].values
    sort_time = time.time() - start_sort
    print(f"   âœ… Sorted in {sort_time:.1f}s")

    print("\n" + "="*60)
    print("âš™ï¸ STEP 2: HYPERPARAMETER TUNING")
    print("="*60)
    print(f"\nUsing Practical Score = 0.7Ã—DR - 0.3Ã—FAR_penalty")
    print(f"(Prioritizes detection while heavily penalizing FAR > 10%)\n")
    
    # Create parameter combinations
    occ_range = [0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.15, 0.2]
    spd_range = [20, 25, 30, 35]
    dur_range = [1, 2, 5, 10]
    
    param_combinations = []
    for occ, spd, dur in [(o, s, d) for o in occ_range for s in spd_range for d in dur_range]:
        param_combinations.append((occ, spd, dur, test_df, y_true))
    
    num_workers = min(20, cpu_count())
    print(f"Testing {len(param_combinations)} parameter combinations using {num_workers} cores...")
    print(f"Estimated time: 1-2 hours for large datasets...\n")
    
    # Run in parallel with simple progress tracking
    start_opt = time.time()
    completed = 0
    results = []
    
    with Pool(processes=num_workers) as pool:
        for result in pool.imap_unordered(evaluate_single_config, param_combinations):
            results.append(result)
            completed += 1
            if completed % 5 == 0 or completed == len(param_combinations):
                elapsed = time.time() - start_opt
                rate = completed / elapsed
                remaining = (len(param_combinations) - completed) / rate if rate > 0 else 0
                print(f"   Progress: {completed}/{len(param_combinations)} configs tested "
                      f"({elapsed/60:.1f}m elapsed, ~{remaining/60:.0f}m remaining)", end='\r')
    
    print()  # New line after progress
    opt_time = time.time() - start_opt
    print(f"\nâœ… Optimization complete in {opt_time/60:.1f} minutes!")
    
    # Sort results
    sorted_results = sorted(results, key=lambda x: x['practical_score'], reverse=True)
    
    # Find BEST OVERALL
    best_overall = sorted_results[0]
    
    # Find BEST WITH FAR < 10%
    results_low_far = [r for r in sorted_results if r['far'] < 10.0]
    best_low_far = results_low_far[0] if results_low_far else None
    
    # Print ALL results in table format
    print(f"\nðŸ“‹ ALL {len(results)} CONFIGURATIONS (sorted by Practical Score):")
    print(f"{'Occ':<6} | {'Spd':<6} | {'Dur':<5} | {'Acc':<6} | {'Prec':<6} | {'Rec':<6} | {'F1':<6} | {'DR(%)':<7} | {'FAR(%)':<7} | {'Score':<7}")
    print("-" * 95)
    
    for r in sorted_results:
        marker = ""
        if r == best_overall:
            marker = "â­BEST "
        elif best_low_far and r == best_low_far:
            marker = "ðŸŽ¯FAR<10 "
        
        print(f"{marker:<8} {r['occ']:<6.3f} | {r['spd']:<6.0f} | {r['dur']:<5.0f} | "
              f"{r['acc']:<6.4f} | {r['prec']:<6.4f} | {r['rec']:<6.4f} | {r['f1']:<6.4f} | "
              f"{r['dr']:<7.2f} | {r['far']:<7.2f} | {r['practical_score']:<7.4f}")
    
    # Summary
    print(f"\n" + "="*60)
    print("ðŸ“Š OPTIMIZATION SUMMARY")
    print("="*60)
    
    print(f"\nâ­ BEST OVERALL (Highest Practical Score):")
    print(f"   Params: Occ={best_overall['occ']:.3f}, Spd={best_overall['spd']:.0f} m/s, Dur={best_overall['dur']}s")
    print(f"   Practical Score: {best_overall['practical_score']:.4f}")
    print(f"   Detection Rate: {best_overall['dr']:.1f}%")
    print(f"   False Alarm Rate: {best_overall['far']:.1f}%")
    print(f"   F1 Score: {best_overall['f1']:.4f}")
    
    if best_low_far:
        print(f"\nðŸŽ¯ BEST WITH FAR < 10% (Production-Ready):")
        print(f"   Params: Occ={best_low_far['occ']:.3f}, Spd={best_low_far['spd']:.0f} m/s, Dur={best_low_far['dur']}s")
        print(f"   Practical Score: {best_low_far['practical_score']:.4f}")
        print(f"   Detection Rate: {best_low_far['dr']:.1f}%")
        print(f"   False Alarm Rate: {best_low_far['far']:.1f}%")
        print(f"   F1 Score: {best_low_far['f1']:.4f}")
    else:
        print(f"\nâš ï¸ No configurations found with FAR < 10%")
    
    # STEP 3: FINAL COMPARISON (using BEST OVERALL)
    print("\n" + "="*60)
    print("ðŸ†š FINAL COMPARISON: Optimized California vs Random Forest")
    print("="*60)
    print(f"   Using BEST OVERALL configuration")
    
    print("\nðŸ”„ Running final evaluation with best parameters...")
    final_cal = CaliforniaAlgorithm(
        threshold_occupancy=best_overall['occ'], 
        threshold_speed=best_overall['spd'], 
        min_duration=best_overall['dur']
    )
    y_pred_cal = final_cal.predict_pre_sorted(test_df)
    cal_res = final_cal.evaluate(y_true, y_pred_cal, save_results=True)

    # Load RF results
    try:
        print("\nðŸ“‚ Loading Random Forest results...")
        with open(config.results_dir / "metrics.json", 'r') as f:
            rf_res = json.load(f)
        
        # Calculate RF's FAR and DR
        rf_cm = rf_res['confusion_matrix']
        rf_far = (rf_cm['fp'] / (rf_cm['fp'] + rf_cm['tn'])) * 100
        rf_dr = (rf_cm['tp'] / (rf_cm['tp'] + rf_cm['fn'])) * 100
        
        print(f"\n{'Metric':<20} {'California':>12} {'Random Forest':>15} {'Improvement':>12}")
        print("-" * 65)
        
        metrics = [
            ('Accuracy', 'accuracy'), 
            ('Precision', 'precision'), 
            ('Recall', 'recall'), 
            ('F1-score', 'f1_score')
        ]
        
        for label, key in metrics:
            cal_val = cal_res['metrics'][key]
            rf_val = rf_res[key]
            diff = rf_val - cal_val
            print(f"{label:<20} {cal_val:>12.4f} {rf_val:>15.4f} {diff:>+12.4f}")
        
        # Operational metrics
        print(f"\n{'Operational Metrics':<20} {'California':>12} {'Random Forest':>15} {'Difference':>12}")
        print("-" * 65)
        print(f"{'Detection Rate %':<20} {best_overall['dr']:>12.2f} {rf_dr:>15.2f} {rf_dr - best_overall['dr']:>+12.2f}")
        print(f"{'False Alarm Rate %':<20} {best_overall['far']:>12.2f} {rf_far:>15.2f} {rf_far - best_overall['far']:>+12.2f}")
        
        # Practical scores
        cal_practical = calculate_practical_score(best_overall['dr'], best_overall['far'])
        rf_practical = calculate_practical_score(rf_dr, rf_far)
        print(f"{'Practical Score':<20} {cal_practical:>12.4f} {rf_practical:>15.4f} {rf_practical - cal_practical:>+12.4f}")
        
        print(f"\nðŸ’¡ Interpretation:")
        if rf_far < 5.0:
            print(f"   âœ… Random Forest has excellent FAR (<5%) - ready for deployment")
        elif rf_far < 10.0:
            print(f"   âœ… Random Forest has good FAR (<10%) - acceptable for deployment")
        else:
            print(f"   âš ï¸ Random Forest FAR is high (>10%) - may need threshold tuning")
        
        if best_overall['far'] < rf_far:
            print(f"   â„¹ï¸ California has lower FAR, but misses {100 - best_overall['dr']:.1f}% of incidents")
        
        print(f"\nðŸ“Š Thesis Conclusion:")
        print(f"   Random Forest achieves {rf_res['f1_score']:.1%} F1-score vs California's {best_overall['f1']:.1%}")
        print(f"   RF detects {rf_dr:.1f}% of incidents with only {rf_far:.2f}% false alarms")
        print(f"   California detects {best_overall['dr']:.1f}% with {best_overall['far']:.2f}% false alarms")
        
    except FileNotFoundError:
        print("\nâš ï¸ Random Forest results not found in results/metrics.json")
        print("   Run model.py first to train and evaluate the Random Forest model")

    return {'best_overall': best_overall, 'best_low_far': best_low_far}


if __name__ == "__main__":
    start_time = time.time()
    best_configs = inspect_and_optimize()
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ Total execution time: {total_time/60:.1f} minutes ({total_time/3600:.1f} hours)")
